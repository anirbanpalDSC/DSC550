{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing Data Science pipeline for text processing\n",
    "\n",
    "In this exercise, you will go back to previous exercises and decompose them into discrete tasks. You will then describe how these tasks connect into workflows. In this exercise, you will only be designing the tasks and the workflows and not be implementing them in code. The actual implementation of these tasks and workflows will be part of your final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries used in this pipeline application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anirban\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import json, re, os, nltk, string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "#### Task 1: Data load\n",
    "\n",
    "Define process to load the corpus data in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\n",
    "\n",
    "def readfiles(dirdata, infile):\n",
    "    \"\"\"\n",
    "    Read the json file(s) as data frame\n",
    "    Args:\n",
    "        data directory, filenames\n",
    "    Output:\n",
    "        pandas dataframe with data from files\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    full_filename = \"%s/%s\" % (dirdata, infile)\n",
    "        \n",
    "    with open(full_filename,'r') as fi:\n",
    "        for line in fi:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    # Create data frame from the json data\n",
    "    outdf = pd.DataFrame(data)\n",
    "    \n",
    "    return outdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Preprocessing\n",
    "\n",
    "Text from the corpus is cleaned and normalized in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaning process\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    ps=PorterStemmer()\n",
    "    \n",
    "    # Create stop words\n",
    "    sw = stopwords.words('english')\n",
    "    \n",
    "    text=text.lower()\n",
    "    text=re.sub('&lt;/?.*?&gt;',' &lt;&gt', text)\n",
    "    text=re.sub(r'\\\\n','',str(text))\n",
    "    text=re.sub(r'@\\w+','',text)\n",
    "    text=re.sub('[^a-zA-Z]',' ',str(text))\n",
    "    text=text.split()\n",
    "    # Remove stop words\n",
    "    text=[ps.stem(word) for word in text if word not in (sw)]\n",
    "    text=' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Tokenization and Count Vectorization\n",
    "\n",
    "Break the sentences in the document into separate words or tokens. \n",
    "Count the number of occurrences each words appears in a document to set the number of features, or in this case words, using CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "#### Task 4: Train Test Split\n",
    "\n",
    "Create test train split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Binary and multinomial model - application and evaluation\n",
    "\n",
    "Apply binary and multinomial model to the data and evaluate accuracy and other model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling\n",
    "\n",
    "Create bag of words topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow creation and execution\n",
    "\n",
    "Create workflow for pipelining the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
