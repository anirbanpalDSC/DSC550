{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Corpus based on the Wikipedia featured articles dataset from the lesson on text processing, select five articles. You will use these articles to calculate the document similarity. Report the similarity between the documents for each distance measure using a table in the following format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           |Article 1  |  Article 2  |  Article 3  |  Article 4  |  Article 5|\n",
    "Article 1  |x.yy       |x.yy         |x.yy         |x.yy         |x.yy       |\n",
    "Article 2  |x.yy       |x.yy         |x.yy         |x.yy         |x.yy       |\n",
    "Article 3  |x.yy       |x.yy         |x.yy         |x.yy         |x.yy       |\n",
    "Article 4  |x.yy       |x.yy         |x.yy         |x.yy         |x.yy       |\n",
    "Article 5  |x.yy       |x.yy         |x.yy         |x.yy         |x.yy       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaration, reading and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create stop words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the json files as data frame\n",
    "def readfiles(dirdata, infile):\n",
    "    data = []\n",
    "\n",
    "    full_filename = \"%s/%s\" % (dirdata, infile)\n",
    "        \n",
    "    with open(full_filename,'r') as fi:\n",
    "        for line in fi:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    # Create data frame from the json data\n",
    "    outdf = pd.DataFrame(data)\n",
    "    \n",
    "    return outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make everything lower case, remove punctuation and newline\n",
    "def cleantext(df):\n",
    "    punc = string.punctuation.replace('<', '').replace('>', '')\n",
    "    pat = re.compile(f'[{punc}]')\n",
    "    \n",
    "    # Change text to lower case\n",
    "    df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "    \n",
    "    # Remove punctuation\n",
    "    df = df.replace(pat, '')\n",
    "    \n",
    "    # Replace newline\n",
    "    df = df.replace(r'\\\\n',' ', regex=True)\n",
    "    \n",
    "    df = df.replace(r'\\\\',' ', regex=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of words from each data frame text column\n",
    "def createwordset(df, col):\n",
    "    results = set()\n",
    "    df[col].str.split().apply(results.update)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define files and location\n",
    "dirdata = 'data/wikipedia/featured-articles'\n",
    "\n",
    "files = ['featured-articles_011.jsonl',\n",
    "         'featured-articles_012.jsonl',\n",
    "         'featured-articles_013.jsonl',\n",
    "         'featured-articles_014.jsonl',\n",
    "         'featured-articles_015.jsonl']\n",
    "\n",
    "# Read files, clean and store as data frame\n",
    "article0 = cleantext(readfiles(dirdata, files[0]))\n",
    "article1 = cleantext(readfiles(dirdata, files[1]))\n",
    "article2 = cleantext(readfiles(dirdata, files[2]))\n",
    "article3 = cleantext(readfiles(dirdata, files[3]))\n",
    "article4 = cleantext(readfiles(dirdata, files[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "article0_txt = pd.DataFrame()\n",
    "article1_txt = pd.DataFrame()\n",
    "article2_txt = pd.DataFrame()\n",
    "article3_txt = pd.DataFrame()\n",
    "article4_txt = pd.DataFrame()\n",
    "\n",
    "article0_txt['texts'] = article0['section_texts'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "article1_txt['texts'] = article1['section_texts'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "article2_txt['texts'] = article2['section_texts'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "article3_txt['texts'] = article3['section_texts'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "article4_txt['texts'] = article4['section_texts'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "txt0 = article0_txt['texts'].str.cat(sep=' ')\n",
    "txt1 = article1_txt['texts'].str.cat(sep=' ')\n",
    "txt2 = article2_txt['texts'].str.cat(sep=' ')\n",
    "txt3 = article3_txt['texts'].str.cat(sep=' ')\n",
    "txt4 = article4_txt['texts'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sets of words to compare\n",
    "d0_words = {}\n",
    "d1_words = {}\n",
    "d2_words = {}\n",
    "d3_words = {}\n",
    "d4_words = {}\n",
    "col = 'texts'\n",
    "\n",
    "d0_words = createwordset(article0_txt, col)\n",
    "d1_words = createwordset(article1_txt, col)\n",
    "d2_words = createwordset(article2_txt, col)\n",
    "d3_words = createwordset(article3_txt, col)\n",
    "d4_words = createwordset(article4_txt, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate jaccard distance\n",
    "def jaccard_distance(d1_words, d2_words):\n",
    "    d1_unique = set(d1_words)\n",
    "    d2_unique = set(d2_words)\n",
    "    num_both = len(d1_unique.intersection(d2_unique))\n",
    "    num_total = len(d1_unique.union(d2_unique))\n",
    "    return num_both/num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Article 11  Article 12  Article 13  Article 14  Article 15\n",
      "Article 11    1.000000    0.318816    0.321713    0.320637    0.312409\n",
      "Article 12    0.318816    1.000000    0.317617    0.316146    0.303839\n",
      "Article 13    0.321713    0.317617    1.000000    0.322966    0.313422\n",
      "Article 14    0.320637    0.316146    0.322966    1.000000    0.322123\n",
      "Article 15    0.312409    0.303839    0.313422    0.322123    1.000000\n"
     ]
    }
   ],
   "source": [
    "# Create final output\n",
    "words = ['Article 11','Article 12','Article 13','Article 14','Article 15']\n",
    "\n",
    "lst_jd = {words[0]:[jaccard_distance(d0_words, d0_words),\n",
    "                  jaccard_distance(d0_words, d1_words),\n",
    "                  jaccard_distance(d0_words, d2_words),\n",
    "                  jaccard_distance(d0_words, d3_words),\n",
    "                  jaccard_distance(d0_words, d4_words)],\n",
    "         words[1]:[jaccard_distance(d1_words, d0_words),\n",
    "                  jaccard_distance(d1_words, d1_words),\n",
    "                  jaccard_distance(d1_words, d2_words),\n",
    "                  jaccard_distance(d1_words, d3_words),\n",
    "                  jaccard_distance(d1_words, d4_words)],\n",
    "         words[2]:[jaccard_distance(d2_words, d0_words),\n",
    "                  jaccard_distance(d2_words, d1_words),\n",
    "                  jaccard_distance(d2_words, d2_words),\n",
    "                  jaccard_distance(d2_words, d3_words),\n",
    "                  jaccard_distance(d2_words, d4_words)],\n",
    "         words[3]:[jaccard_distance(d3_words, d0_words),\n",
    "                  jaccard_distance(d3_words, d1_words),\n",
    "                  jaccard_distance(d3_words, d2_words),\n",
    "                  jaccard_distance(d3_words, d3_words),\n",
    "                  jaccard_distance(d3_words, d4_words)],\n",
    "         words[4]:[jaccard_distance(d4_words, d0_words),\n",
    "                  jaccard_distance(d4_words, d1_words),\n",
    "                  jaccard_distance(d4_words, d2_words),\n",
    "                  jaccard_distance(d4_words, d3_words),\n",
    "                  jaccard_distance(d4_words, d4_words)],} \n",
    "  \n",
    "# Create DataFrame \n",
    "df_jd = pd.DataFrame(lst_jd)\n",
    "\n",
    "df_jd.set_index([words], inplace = True)\n",
    "\n",
    "print(df_jd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize using tf/idf\n",
    "def get_vectors(*strs):\n",
    "    text = [t for t in strs]\n",
    "    vectorizer = TfidfVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "\n",
    "# Create cosine similarity\n",
    "def get_cosine_sim(*strs):\n",
    "    vectors = [t for t in get_vectors(*strs)]\n",
    "    return cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.91942789, 0.91489015, 0.91551904, 0.89558325],\n",
       "       [0.91942789, 1.        , 0.91315338, 0.91265633, 0.88628739],\n",
       "       [0.91489015, 0.91315338, 1.        , 0.91370187, 0.89143987],\n",
       "       [0.91551904, 0.91265633, 0.91370187, 1.        , 0.90381425],\n",
       "       [0.89558325, 0.88628739, 0.89143987, 0.90381425, 1.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply cosine similarity\n",
    "cosim = get_cosine_sim(txt0, txt1, txt2, txt3, txt4)\n",
    "cosim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Article 11  Article 12  Article 13  Article 14  Article 15\n",
      "Article 11    1.000000    0.919428    0.914890    0.915519    0.895583\n",
      "Article 12    0.919428    1.000000    0.913153    0.912656    0.886287\n",
      "Article 13    0.914890    0.913153    1.000000    0.913702    0.891440\n",
      "Article 14    0.915519    0.912656    0.913702    1.000000    0.903814\n",
      "Article 15    0.895583    0.886287    0.891440    0.903814    1.000000\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe from ndarray so that we can output the result in the required format\n",
    "df_cosim = pd.DataFrame({'Article 11':cosim[:,0],\n",
    "                         'Article 12':cosim[:,1],\n",
    "                         'Article 13':cosim[:,2],\n",
    "                         'Article 14':cosim[:,3],\n",
    "                         'Article 15':cosim[:,4]})\n",
    "df_cosim.set_index([words], inplace = True)\n",
    "\n",
    "print(df_cosim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of code**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
