{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Corpus based on the Wikipedia featured articles dataset from the lesson on text processing, select five articles. You will use these articles to calculate the document similarity. Report the similarity between the documents for each distance measure using a table in the following format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Article 1    Article 2    Article 3    Article 4    Article 5\n",
    "Article 1    x.yy    x.yy    x.yy    x.yy    x.yy\n",
    "Article 2    x.yy    x.yy    x.yy    x.yy    x.yy\n",
    "Article 3    x.yy    x.yy    x.yy    x.yy    x.yy\n",
    "Article 4    x.yy    x.yy    x.yy    x.yy    x.yy\n",
    "Article 5    x.yy    x.yy    x.yy    x.yy    x.yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create stop words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the json files as data frame\n",
    "def readfiles(dirdata, infile):\n",
    "    data = []\n",
    "\n",
    "    full_filename = \"%s/%s\" % (dirdata, infile)\n",
    "        \n",
    "    with open(full_filename,'r') as fi:\n",
    "        for line in fi:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    # Create data frame from the json data\n",
    "    outdf = pd.DataFrame(data)\n",
    "    \n",
    "    return outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make everything lower case, remove punctuation and newline\n",
    "def cleantext(df):\n",
    "    punc = string.punctuation.replace('<', '').replace('>', '')\n",
    "    pat = re.compile(f'[{punc}]')\n",
    "    \n",
    "    # Change text to lower case\n",
    "    df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "    \n",
    "    # Remove punctuation\n",
    "    df = df.replace(pat, '')\n",
    "    \n",
    "    # Replace newline\n",
    "    df = df.replace(r'\\\\n',' ', regex=True)\n",
    "    \n",
    "    df = df.replace(r'\\\\',' ', regex=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of words from each data frame text column\n",
    "def createwordset(df, col):\n",
    "    results = set()\n",
    "    df[col].str.split().apply(results.update)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate jaccard distance\n",
    "def jaccard_distance(d1_words, d2_words):\n",
    "    d1_unique = set(d1_words)\n",
    "    d2_unique = set(d2_words)\n",
    "    num_both = len(d1_unique.intersection(d2_unique))\n",
    "    num_total = len(d1_unique.union(d2_unique))\n",
    "    return num_both/num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define files and location\n",
    "dirdata = 'data/wikipedia/featured-articles'\n",
    "\n",
    "files = ['featured-articles_011.jsonl',\n",
    "         'featured-articles_012.jsonl',\n",
    "         'featured-articles_013.jsonl',\n",
    "         'featured-articles_014.jsonl',\n",
    "         'featured-articles_015.jsonl']\n",
    "\n",
    "# Read files, clean and store as data frame\n",
    "article0 = cleantext(readfiles(dirdata, files[0]))\n",
    "article1 = cleantext(readfiles(dirdata, files[1]))\n",
    "article2 = cleantext(readfiles(dirdata, files[2]))\n",
    "article3 = cleantext(readfiles(dirdata, files[3]))\n",
    "article4 = cleantext(readfiles(dirdata, files[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "article0_txt = pd.DataFrame()\n",
    "article1_txt = pd.DataFrame()\n",
    "article2_txt = pd.DataFrame()\n",
    "article3_txt = pd.DataFrame()\n",
    "article4_txt = pd.DataFrame()\n",
    "\n",
    "article0_txt['texts'] = article0['section_texts'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "article1_txt['texts'] = article1['section_texts'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "article2_txt['texts'] = article2['section_texts'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "article3_txt['texts'] = article3['section_texts'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "article4_txt['texts'] = article4['section_texts'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sets of words to compare\n",
    "d0_words = {}\n",
    "d1_words = {}\n",
    "d2_words = {}\n",
    "d3_words = {}\n",
    "d4_words = {}\n",
    "col = 'texts'\n",
    "\n",
    "d0_words = createwordset(article0_txt, col)\n",
    "d1_words = createwordset(article1_txt, col)\n",
    "d2_words = createwordset(article2_txt, col)\n",
    "d3_words = createwordset(article3_txt, col)\n",
    "d4_words = createwordset(article4_txt, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   d0_words  d1_words  d2_words  d3_words  d4_words\n",
      "0  1.000000  0.318816  0.321713  0.320637  0.312409\n",
      "1  0.318816  1.000000  0.317617  0.316146  0.303839\n",
      "2  0.321713  0.317617  1.000000  0.322966  0.313422\n",
      "3  0.320637  0.316146  0.322966  1.000000  0.322123\n",
      "4  0.312409  0.303839  0.313422  0.322123  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Create final output\n",
    "words = ['d0_words','d1_words','d2_words','d3_words','d4_words']\n",
    "\n",
    "lst_jd = {words[0]:[jaccard_distance(d0_words, d0_words),\n",
    "                  jaccard_distance(d0_words, d1_words),\n",
    "                  jaccard_distance(d0_words, d2_words),\n",
    "                  jaccard_distance(d0_words, d3_words),\n",
    "                  jaccard_distance(d0_words, d4_words)],\n",
    "         words[1]:[jaccard_distance(d1_words, d0_words),\n",
    "                  jaccard_distance(d1_words, d1_words),\n",
    "                  jaccard_distance(d1_words, d2_words),\n",
    "                  jaccard_distance(d1_words, d3_words),\n",
    "                  jaccard_distance(d1_words, d4_words)],\n",
    "         words[2]:[jaccard_distance(d2_words, d0_words),\n",
    "                  jaccard_distance(d2_words, d1_words),\n",
    "                  jaccard_distance(d2_words, d2_words),\n",
    "                  jaccard_distance(d2_words, d3_words),\n",
    "                  jaccard_distance(d2_words, d4_words)],\n",
    "         words[3]:[jaccard_distance(d3_words, d0_words),\n",
    "                  jaccard_distance(d3_words, d1_words),\n",
    "                  jaccard_distance(d3_words, d2_words),\n",
    "                  jaccard_distance(d3_words, d3_words),\n",
    "                  jaccard_distance(d3_words, d4_words)],\n",
    "         words[4]:[jaccard_distance(d4_words, d0_words),\n",
    "                  jaccard_distance(d4_words, d1_words),\n",
    "                  jaccard_distance(d4_words, d2_words),\n",
    "                  jaccard_distance(d4_words, d3_words),\n",
    "                  jaccard_distance(d4_words, d4_words)],} \n",
    "  \n",
    "# Create DataFrame \n",
    "df_jd = pd.DataFrame(lst_jd)\n",
    "\n",
    "df_jd.set_index(words)\n",
    "\n",
    "print(df_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set source data location (as per assignment, since we need only 5 files, the first 5 files are used)\n",
    "#dirData = 'data/wikipedia/featured-articles'\n",
    "\n",
    "# Create list to hold initial data\n",
    "#data = []\n",
    "\n",
    "# Loop though all the jsonl files and append to the list\n",
    "#for file in os.listdir(dirData):\n",
    " #   full_filename = \"%s/%s\" % (dirData, file)\n",
    "  #  with open(full_filename,'r') as fi:\n",
    "   #     for line in fi:\n",
    "    #        data.append(json.loads(line))\n",
    "\n",
    "# Create data frame from the json data\n",
    "#articles = pd.DataFrame(data)\n",
    "\n",
    "# Examine head of the data frame\n",
    "#articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
