{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an earlier lesson, you created text classification models using logistic regression and Naive Bayes. In this exercise, you will expand on those models and learn how to properly evaluate the performance of those models and select the best model.\n",
    "\n",
    "### 1. Evaluating Binary Classifiers\n",
    "\n",
    "#### a. Model Metrics\n",
    "\n",
    "Using the same model parameters you used in the text classification exercise, the model’s accuracy, ROC AUC, precision, recall, and F1-score.\n",
    "\n",
    "<img src=\"files/Images/ex9-1.jpg\">\n",
    "\n",
    "#### b. Cross Validation\n",
    "\n",
    "Using the logistic regression model with the L2 penalty, calculate the cross-validation score of the model’s ROC AUC using three-fold cross validation.\n",
    "\n",
    "### 2. Evaluating Multi-class Classifiers\n",
    "\n",
    "Using the same data you used for the multi-class text classifier in the text classification lesson, calculate the following model metrics.\n",
    "\n",
    "#### a. Model Metrics\n",
    "\n",
    "<img src=\"files/Images/ex9-2.jpg\">\n",
    "\n",
    "#### b. Cross Validation\n",
    "\n",
    "Using the logistic regression model with the L2 penalty, calculate the cross-validation score of the model’s F1 score using three-fold cross validation.\n",
    "\n",
    "### 3. Hyperparameter Selection\n",
    "\n",
    "Use the GridSearchCV function to select the best binary classification model. For this problem, only consider the logistic regression model. For the search parameters try both L1 and L2 with C values of np.logspace(0, 4, 10). Report the cross-validation score and parameters of the best model.\n",
    "\n",
    "<img src=\"files/Images/ex9-4.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centralized import of all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, json, re, pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, auc, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove punctuations and special characters, makes lower case\n",
    "    Args: text \n",
    "    Output: text\n",
    "    \"\"\"\n",
    "    \n",
    "    text=text.lower()\n",
    "    text=re.sub('&lt;/?.*?&gt;',' &lt;&gt', text)\n",
    "    text=re.sub('\\\\d|\\\\W+|_',' ',text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Create stop words list\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bernoulli NB classifier\n",
    "bnb = BernoulliNB(class_prior=[0.25, 0.5])\n",
    "\n",
    "# Create Multinomial NB classifier\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Create Logistic Regression classifier (for Penalty l1 and l2)\n",
    "lr1 = LogisticRegression(penalty = 'l1', solver = 'saga', max_iter = 100, multi_class = 'auto')\n",
    "lr2 = LogisticRegression(penalty = 'l2', solver = 'saga', max_iter = 100, multi_class = 'auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    \"\"\"\n",
    "    Take a json file location and\n",
    "    read the file into a pandas data frame\n",
    "    Args: full path to file\n",
    "    Returns: pandas dataframe with data from file\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        \n",
    "    # convert to data frame\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluating Binary Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 950000 entries, 0 to 949999\n",
      "Data columns (total 2 columns):\n",
      "con    950000 non-null int64\n",
      "txt    950000 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 10.9+ MB\n",
      "Size:  950000 \n",
      " Shape:  None \n",
      " Categories:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "# read controversy data\n",
    "\n",
    "#cat_df = read_data('data/reddit/categorized-comments.jsonl')\n",
    "con_df = read_data('data/reddit/controversial-comments.jsonl')\n",
    "\n",
    "# check size, structure and categories\n",
    "\n",
    "print('Size: ', len(con_df), '\\n',\n",
    "      'Shape: ', con_df.info(), '\\n',\n",
    "      'Categories: ', con_df.con.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>deleted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>johnson is on the rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>well her being a woman might be number   but i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i don t see how the dems are currently doing i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i don t want to say more extreme but when i th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   con                                                txt\n",
       "0    0                                           deleted \n",
       "1    0                            johnson is on the rise \n",
       "2    0  well her being a woman might be number   but i...\n",
       "3    0  i don t see how the dems are currently doing i...\n",
       "4    0  i don t want to say more extreme but when i th..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since the size is humongus, I will take sample of the 2 categories. \n",
    "# by trial, sample of 50000 from each category can be easily handled by my machine\n",
    "\n",
    "size = 50000    # sample size\n",
    "replace = True  # with replacement\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "\n",
    "controversy = con_df.groupby('con', as_index=False).apply(fn)\n",
    "\n",
    "# free up memory\n",
    "\n",
    "del con_df\n",
    "\n",
    "controversy['txt'] = controversy['txt'].apply(lambda x:clean_text(x))\n",
    "controversy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "controversy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train, X_test, Y_train, Y_test, classifier, binary):\n",
    "    \"\"\"\n",
    "    Evaluate model and return model scores. In the output, \n",
    "    AUC is calculated for binary model only\n",
    "    Args: feature - train, feature - test, target - train, target - test,\n",
    "          classifier (BNB, LR1, LR2, MNB), binary (Y, N)\n",
    "    Output: model scores (accuracy, conf_matrix, auc, \n",
    "            precision, recall, fscore, support)            \n",
    "    \"\"\"  \n",
    "    \n",
    "    # Bernoulli's Naive Bayes\n",
    "    \n",
    "    if classifier == 'BNB':\n",
    "        BNB = bnb.fit(X_train, Y_train)\n",
    "        \n",
    "        Y_pred = BNB.predict(X_test)\n",
    "               \n",
    "        conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "        \n",
    "        accuracy = accuracy_score(Y_test, Y_pred)        \n",
    "        \n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(Y_test, Y_pred, average='weighted')\n",
    "        \n",
    "        if binary == 'Y':\n",
    "            auc = roc_auc_score(Y_test, Y_pred)\n",
    "            return accuracy, auc, precision, recall, fscore\n",
    "        else:\n",
    "            return accuracy, precision, recall, fscore\n",
    "        \n",
    "    # Logistic Regression with penalty L1\n",
    "        \n",
    "    elif classifier == 'LR1':\n",
    "        LR1 = lr1.fit(X_train, Y_train)\n",
    "        \n",
    "        Y_pred = LR1.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(Y_test, Y_pred)        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "                \n",
    "        accuracy = accuracy_score(Y_test, Y_pred)\n",
    "        \n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(Y_test, Y_pred, average='weighted')\n",
    "                \n",
    "        if binary == 'Y':\n",
    "            auc = roc_auc_score(Y_test, Y_pred)\n",
    "            return accuracy, auc, precision, recall, fscore\n",
    "        else:\n",
    "            return accuracy, precision, recall, fscore\n",
    "        \n",
    "    # Logistic Regression with penalty L2\n",
    "        \n",
    "    elif classifier == 'LR2':\n",
    "        LR2 = lr2.fit(X_train, Y_train)\n",
    "        \n",
    "        Y_pred = LR2.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(Y_test, Y_pred)        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "       \n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(Y_test, Y_pred, average='weighted')\n",
    "        \n",
    "        if binary == 'Y':\n",
    "            auc = roc_auc_score(Y_test, Y_pred)\n",
    "            return accuracy, auc, precision, recall, fscore\n",
    "        else:\n",
    "            return accuracy, precision, recall, fscore\n",
    "        \n",
    "    # Multinomial Naive Bayes\n",
    "        \n",
    "    elif classifier == 'MNB':\n",
    "        MNB = mnb.fit(X_train, Y_train)\n",
    "        \n",
    "        Y_pred = MNB.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(Y_test, Y_pred)        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "                \n",
    "        #auc = roc_auc_score(Y_test, Y_pred)\n",
    "        \n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(Y_test, Y_pred, average='weighted')\n",
    "        \n",
    "        return accuracy, precision, recall, fscore\n",
    "    else:\n",
    "        print('Wrong calssifier')\n",
    "\n",
    "\n",
    "# function for k-fold cross validation\n",
    "        \n",
    "def evaluate_crossval(X_train, X_test, Y_train, Y_test, classifier, fold):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross validation of the LR model\n",
    "    Args: feature - train, feature - test, target - train, target - test,\n",
    "          classifier (LR1, LR2), k-fold\n",
    "    Output: Cross validation score      \n",
    "    \"\"\"\n",
    "    \n",
    "    if classifier == 'LR1':\n",
    "        lr = lr1.fit(X_train, Y_train)\n",
    "    elif classifier == 'LR2':\n",
    "        lr = lr2.fit(X_train, Y_train)\n",
    "    else:\n",
    "        print('Wrong calssifier')\n",
    "    \n",
    "    cvs = cross_val_score(estimator = lr, X = X_train, y = Y_train, n_jobs=-1, cv = fold, scoring = 'roc_auc')\n",
    "    \n",
    "    return cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the feature matrix\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# create target and sample\n",
    "\n",
    "X_bc = cv.fit_transform(controversy['txt'])\n",
    "Y_bc = controversy['con']\n",
    "\n",
    "# create train test split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_bc, Y_bc, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional explanation\n",
    "I am getting non-convergence warning in the Logistic Regression model. I tried to increase the max iteration upto 500. Increasing it more might remove the warning but my CPU freezes during the process. Hence, I left the iteration at default 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (L1)\n",
      "Accuracy:  0.61724\n",
      "AUC:  0.6172557976593469\n",
      "Precision:  0.6172748543614726\n",
      "Recall:  0.61724\n",
      "F1-Score:  0.6172424858008753\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression (L1)\n",
    "\n",
    "accuracy, auc, precision, recall, fscore = evaluate_model(X_train, X_test, Y_train, Y_test, 'LR1', 'Y')\n",
    "print('Logistic Regression (L1)')\n",
    "print('Accuracy: ', accuracy)\n",
    "print('AUC: ', auc)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1-Score: ', fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (L2)\n",
      "Accuracy:  0.61776\n",
      "AUC:  0.617862432480251\n",
      "Precision:  0.6180164192307499\n",
      "Recall:  0.61776\n",
      "F1-Score:  0.6176697281925775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anirban.pal\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression (L2)\n",
    "\n",
    "accuracy, auc, precision, recall, fscore = evaluate_model(X_train, X_test, Y_train, Y_test, 'LR2', 'Y')\n",
    "print('Logistic Regression (L2)')\n",
    "print('Accuracy: ', accuracy)\n",
    "print('AUC: ', auc)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1-Score: ', fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Accuracy:  0.56816\n",
      "AUC:  0.570401154051976\n",
      "Precision:  0.6457365735617245\n",
      "Recall:  0.56816\n",
      "F1-Score:  0.5045516540705048\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "accuracy, auc, precision, recall, fscore = evaluate_model(X_train, X_test, Y_train, Y_test, 'BNB', 'Y')\n",
    "print('Naive Bayes')\n",
    "print('Accuracy: ', accuracy)\n",
    "print('AUC: ', auc)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1-Score: ', fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anirban.pal\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.65338806, 0.65619237, 0.66228931])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating 3-fold cross validation for logistic regression with L2 penalty\n",
    "\n",
    "evaluate_crossval(X_train, X_test, Y_train, Y_test, 'LR2', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluating Multi-class Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2347476 entries, 0 to 2347475\n",
      "Data columns (total 2 columns):\n",
      "cat    object\n",
      "txt    object\n",
      "dtypes: object(2)\n",
      "memory usage: 17.9+ MB\n",
      "Size:  2347476 \n",
      " Shape:  None \n",
      " Categories:  ['sports' 'science_and_technology' 'video_games' 'news']\n"
     ]
    }
   ],
   "source": [
    "# read category data\n",
    "\n",
    "cat_df = read_data('data/reddit/categorized-comments.jsonl')\n",
    "\n",
    "# check size, structure and categories\n",
    "\n",
    "print('Size: ', len(cat_df), '\\n',\n",
    "      'Shape: ', cat_df.info(), '\\n',\n",
    "      'Categories: ', cat_df.cat.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>cocaine abuse in addition to epo abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>saying that   people can enter from x country ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>deleted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>yet people like the rothschilds literally have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>dj khaled sharia raw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat                                                txt\n",
       "0  news            cocaine abuse in addition to epo abuse \n",
       "1  news  saying that   people can enter from x country ...\n",
       "2  news                                           deleted \n",
       "3  news  yet people like the rothschilds literally have...\n",
       "4  news                               dj khaled sharia raw"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since the size is humongus, I will take sample of each categories. \n",
    "# by trial, sample of 10000 from each category can be easily handled by my machine\n",
    "\n",
    "size = 10000    # sample size\n",
    "replace = True  # with replacement\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "\n",
    "category = cat_df.groupby('cat', as_index=False).apply(fn)\n",
    "\n",
    "# free up memory\n",
    "\n",
    "del cat_df\n",
    "\n",
    "category['txt'] = category['txt'].apply(lambda x:clean_text(x))\n",
    "category.reset_index(drop=True, inplace=True)\n",
    "\n",
    "category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (L1)\n",
      "Accuracy:  0.56816\n",
      "Precision:  0.614903745678144\n",
      "Recall:  0.61472\n",
      "F1-Score:  0.6146652671139686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anirban.pal\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# create the feature matrix\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# create target and sample for multi-class model \n",
    "\n",
    "X_mc = cv.fit_transform(category['txt'])\n",
    "Y_mc = category['cat']\n",
    "\n",
    "# Logistic Regression (L1)\n",
    "\n",
    "auc, precision, recall, fscore = evaluate_model(X_train, X_test, Y_train, Y_test, 'LR1', 'N')\n",
    "print('Logistic Regression (L1)')\n",
    "print('Accuracy: ', accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1-Score: ', fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64960309, 0.65125267, 0.65508894])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating 3-fold cross validation for logistic regression with L2 penalty\n",
    "\n",
    "evaluate_crossval(X_train, X_test, Y_train, Y_test, 'LR2', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression\n",
    "\n",
    "lr = LogisticRegression(solver = 'saga', max_iter = 100)\n",
    "\n",
    "# Create regularization penalty space\n",
    "\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create hyperparameter options\n",
    "\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "\n",
    "grid = GridSearchCV(lr, hyperparameters, cv=5, verbose=0, n_jobs=3)\n",
    "\n",
    "# Fit grid search\n",
    "\n",
    "best_model = grid.fit(X_bc, Y_bc)\n",
    "\n",
    "# View best hyperparameters\n",
    "\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "\n",
    "#grid.fit(Xtrain, ytrain, verbose=True)\n",
    "# make predictions for test data\n",
    "#ypred = clf.predict(Xtest)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "#evaluate predictions\n",
    "#accuracy = accuracyscore(ytest, predictions)\n",
    "\n",
    "#print(\"Accuracy: %.2f%\" % (accuracy * 100.0))\n",
    "#output: Accuracy: 0.93\n",
    "#print(\"Accuracy Score : \" + str(grid.score(Xtest,ytest)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
